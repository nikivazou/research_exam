\newcommand\trefc[4]{\ensuremath{\langle\{#1 : #2  \mid #3 \}\rangle^{#4}}}

\newcommand\tlabel{T-Label}
\newcommand\tcast{T-Cast}

\newcommand\cbase{C-Base}
\newcommand\cfunction{C-Fun}


\newcommand\elabel[1]{\ensuremath{\Uparrow \ #1}}
\newcommand\ecast[3]{\ensuremath{\langle #1 \Rightarrow #2 \rangle^{#3}}}

\newcommand\isCompatible[2]{\ensuremath{ #1 \Vert #2 }}



\section{Interactive theorem Proving}

% intro

One approach to verify that a program satisfies some 
specifications is to statically prove them.
% 
This approach is used by \textit{interactive theorem provers}, 
such as Coq, Agda and Isabelle,
that allow the 
expression of mathematical assertions, 
mechanically check proofs of these assertions, 
help to find formal proofs, 
and extract a certified program from 
the constructive proof of its formal specification.

% overview 

As an example, consider a function @pred@ in Coq, 
that computes the predecessor of a positive number,
and has type signature

@pred :: s:{n : nat | n > 0}  -> {v:nat | s = S v}@

This type signature says that if @pred@ is called with a 
positive number @s@, it will return @s@'s predecessors. 
There are two different assertions that should be proved:
\begin{itemize}
\item The result of the function is the predecessor of the argument.
At @pred@'s definitions the programmer should provide a proof
that this assertion is indeed satisfied.
\item The argument is a positive number.
At each call side of @pred@, 
the user should provide a proof that 
its argument is positive.
\end{itemize}

% implementation

In more details @pred@ function can be defined in Coq as follows \cite{cpdt}  
$$\centering
\begin{tabular}{c}
\begin{codeCoq}
Definition pred (s : {n : nat | n > 0}) : {m : nat | proj1_sig s = S m} :=
  match s return {m : nat | proj1_sig s = S m} with
    | exist 0 pf => match zgtz pf with end
    | exist (S n') pf => exist _ n' (eq_refl (S n'))
  end.
\end{codeCoq}
\end{tabular}
$$

where the refinement type is syntactic sugar,
defined in the standard library, for the type family @sig@:

$$\centering
\begin{tabular}{c}
\begin{codeCoq}
Inductive sig (A : Type) (P : A -> Prop) : Type :=
    exist : forall x : A, P x -> sig P
Notation
  "{ x : A | P }" := sig (fun x : A => P)
\end{codeCoq}
\end{tabular}
$$

The function @pred@ takes an argument @s@ which has two components:
a natural number @n@
and a proof @pf@ that this number is positive.
Then, there is a case analysis on @s@ and if @n@ is zero, then 
the proof @pf@ is used to reach a contradiction; thus this case can not occur.
Otherwise, @n@ has a predecessor, say @n'@ and 
the function returns @n'@ combined with a proof that its successor is 
equal to @n@. This proof is constructed by applying @eq_relf@, the only constructor
of equality to @S n'@.


\begin{comment}
where zgtz

\begin{verbatim}
Lemma zgtz : 0 > 0 -> False.
  crush.
Qed.

Inductive eq (A:Type) (x:A) : A -> Prop :=
    eq_refl : x = x :>A
\end{verbatim}
\end{comment}

As we said, at the call side of @pred@, the programmer should provide
both the argument and a proof that it is positive.
As an example, we can have 

$$\centering
\begin{tabular}{c}
\begin{code}
pred (exist _ 2 two_gt0)
\end{code}
\end{tabular}
$$
 
where @two_gt0@ is a proof that two is greater than zero.

\begin{comment}
Theorem two_gt0 : 2 > 0.
  crush.
Qed.
\end{comment}

Even this example seems tedious, 
interactive theorem prooving can be simplified
using inference and tacticks. 
%
Though, the user still needs to provide proofs.
%
We will discuss other systems, which remove this burden from the user.

\subsection{Contracts}

% intro

Another approach to verify that a program satisfies some 
assertions is dynamically check them.
% 
These assertions are called \textit{contracts}, i.e., 
dynamically enforced pre- and post assertions that
define formal, precise and verifiable interface specifications 
for software components.
%	
Their use in programming languages dates back to the 1970s; 
when Eiffel \cite{Eiffel}, an object-oriented programming language,
totally adopted assertions and developed the 
``Design by Contract'' philosophy \cite{Meyer92}.

Contracts are of the form:
\trefc{v}{\tau}{p}{l}
and describe the values $v$, of type $\tau$
that satisfy the predicate $p$. 
The $l$ superscript is a \textit{blame label}, used to
identify the source of failures.
%
As an example, 
consider a contract for positive integers \trefc{v}{Int}{v>0}{l}
applied to two values, $2$ and $0$: 
\begin{align*}
\trefc{v}{Int}{v>0}{l'}&\ 2 &&\rightarrow 2 \\
\trefc{v}{Int}{v>0}{l}&\ 0  &&\rightarrow \Uparrow l
\end{align*}

If the check succeeds, as in the case for $2$,
then the application will return the value, so the first 
application just returns $2$.
If it fails, then the entire program will ``blame'' the label $l$,
raising an uncatchable exception  $\Uparrow l$, pronounced ``blame $l$''.

% higher-order 
Assigning blame for contractual violations in higher-order languages
is complex:
%
The boundaries between cooperating components are more obscure 
than in the world with only first-order functions. 
A function may invoke a function passed to it at its call side.
Accordingly, the blame for a corresponding contract violation must 
lie with the supplier of the bad value, 
no matter if the bad value was passed by directly applying 
a function or by applying a base value.
%

In 2002, Findler and Felleisen in \cite{Findler02} were the first to
to create a system for higher order languages with contracts.
%
In their system, the blame is properly assigned in the 
higher-order components of the program via the 
``variance-contravariance`` rule.
% Other features
Moreover,
they allow dependent contracts, i.e. 
contracts that have the form of a dependent function type, 
where the result can depend on the argument.
Finally,
they treat contracts as first class values, 
i.e., contracts are values that can be passed to and from functions

In 2004, Blume and McAllester, at \cite{BlumeM06} noted that 
in Findler and Felleisen system,
the concept of contract satisfaction had not actually been defined
formally, and they proved that their system is indeed
sound and complete, 
%
The contract system is sound if whenever
the algorithm blames a contract declaration, 
that contract declaration is actually wrong.
%
Conversely, it is complete if blame on a expression is explained
by the fact that the expression violates one of it contract interfaces.

% Contracts and refinement types 
Findler and Felleisen's work sparked a great interest
in contracts, and in the following years a variety
of related systems have been studied. 
Broadly, these come in two different sorts. 
%
In systems with \textit{latent contracts}, types and contracts
are orthogonal features. Examples of this style include Findler
and Felleisen's original system, Hinze et al. \cite{Hinze06}, Blume and
McAllester \cite{BlumeM06}, Chitil and Huch \cite{ChitilH07}, Guha et al. \cite{GuhaMFK07},
and Tobin-Hochstadt and Felleisen \cite{Tobin-HochstadtF08}. 
By contrast, \textit{manifest contracts} are integrated into the type system, 
which tracks, for each value, the most recently checked contract. 
Hybrid types \cite{flanagan06} are a well-known example in this style; others include the
work of Ou et al. \cite{Ou2004}, Wadler and Findler \cite{WadlerF09}, and Gronski
et al. \cite{Gronski06}, Belo et al. \cite{Greenberg11} and Grennberg et al. \cite{Greenberg12}.

In the next subsection we will discuss manifest contracts and
how we can extend out core language to support them.

\subsubsection{Manifest Contracts}

Manifest Contracts Systems, as presented in \cite{Greenberg12}, use casts,
\ecast{\tau_s}{\tau_t}{l}
to convert values from the source type $\tau_s$ 
to the target type $\tau_t$ and raise \elabel{l}
if the cast fails.

As an example, consider a cast from integers to positives:

$$
\ecast{Int}{\{v:Int | v > 0\}}{l} \ n
$$

The system should statically verify that the value $n$ is 
of the source type $Int$.
After the cast, the program can treat this value as
if it has the target type $\{v:Int | v > 0\}$.
At run-time, a check will be made that $n$ 
is actually a positive integer and if it fails it will raise \elabel{l}.

To generalize, for base contracts, a cast will behave just like a check on the target type: 
applied to $n$, the cast either returns $n$ or raises \elabel{l} . 

A function cast 
$$
(\ecast{\tau_{11} \rightarrow \tau_{12}}{\tau_{21} \rightarrow \tau_{22}}{l}
 \ f ) \ v
$$
will reduce to 
$$
\ecast{\tau_{12}}{\tau_{22}}{l}\ (f \ ((\ecast{\tau_{21}}{\tau_{11}}{l}) \ v))
$$

wrapping the argument v in a (contravariant) cast between the domain types
and wrapping the result of the application in a (covariant) cast between the
codomain types.

To better understand how function casts work lets go back to 
our running example, \texttt{pred}:

$$
\texttt{pred} ::n:\{v:Int | v > 0\} -> \{v:Int | v = n - 1\} 
$$

To get this type signature for \texttt{pred}, we have to wrap the function's definition 
in a type cast:

\begin{align*}
& \texttt{pred'}\ x= x - 1 \\
& \texttt{pred}  = \ecast{Int \rightarrow Int}{x:\{v:Int | v > 0\} -> \{v:Int | v = x - 1\}}{lpred} \ \texttt{pred'}
\end{align*}


Now, when we apply a positive number, say $2 :: \{v:Int | v > 0\}$, we will have the following computation:

\begin{align*}
\texttt{pred}\ 2 &=
(\ecast{Int \rightarrow Int}{x:\{v:Int | v > 0\} -> \{v:Int | v = x - 1\}}{lpred} \ \texttt{pred'})\ 2 \\
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ (\texttt{pred'}\ (\ecast{\{v:Int | v > 0\}}{Int}{lpred} \ 2)) \\ 
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ (\texttt{pred'}\ 2) \\ 
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ 1 \\ 
&\rightarrow^\star 1 \\ 
\end{align*}

Thus, if \texttt{pred'} was not returning the correct value, the program would raise a blame:
\begin{align*}
& \texttt{pred'}\ x= 0 \\
\end{align*}

\begin{align*}
\texttt{pred}\ 2 &=
(\ecast{Int \rightarrow Int}{x:\{v:Int | v > 0\} -> \{v:Int | v = x - 1\}}{lpred} \ \texttt{pred'})\ 2 \\
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ (\texttt{pred'}\ (\ecast{\{v:Int | v > 0\}}{Int}{lpred} \ 2)) \\ 
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ (\texttt{pred'}\ 2) \\ 
&\rightarrow^\star (\ecast{Int}{\{v:Int | v = 2 - 1\}}{lpred}) \ 0 \\ 
&\rightarrow^\star \elabel{lpred} \\ 
\end{align*}




% testing the argument
You may notice, that in both cases, \texttt{pred'} is applied to a ``casted'' value
$\ecast{\{v:Int | v > 0\}}{Int}{lpred} \ 2$.
This is a downcast: the system statically knows the source type of $2$, i.e., 
$\{v:Int | v > 0\}$ and at runtime it is checked that $2$ is an $Int$.
Thus, for an application to statically typecheck, the argument should 
have a refined type and the only way to get a refined type is though a cast.
But, if we cast a non-positive value to be positive,
then this cast will fail:

\begin{align*}
\texttt{pred} \ ((\ecast{Int}{\{v:Int | v > 0\}}{zero})\ 0) &= \elabel{zero}
\end{align*}


\subsubsection{Formal Language}
Lets now extend our core calculus, so that it supports manifest contracts.
%
In the expressions of our language we should add a blaming expression
and a type casting.
As a refinement the language can use any core expression.
Everything else remains unchanged.


In the typing judgements we add two rules:
a blame expression can have any well formed types, 
while a type casting expression behaves as a function from the 
source to the target type.
For a casting expression to typecheck, both types should be 
well formed and compatible, i.e., their unrefined types should be 
the same. We check this through a new compatibility judgement.

\begin{figure}[ht!]
\centering
$$
\begin{array}{rrcl}
\emphbf{Expressions} \quad 
  & e 
  & ::= 
  & 		 \dots
  \spmid \elabel{l} 
  \spmid \ecast{\tau}{\tau}{l} 
  \\[0.05in] 

\emphbf{Predicates} \quad 
  & p
  & ::= 
  &		e
  \\[0.05in] 

\end{array}
$$
\caption{\textbf{Syntax of Expressions, Types and Schemas}}
\label{fig:syntax}
\end{figure}


\begin{figure}[ht!]

\medskip \judgementHead{Compatibility}{$\isCompatible{\tau_1}{\tau_2}$}
$$\begin{array}{cc}

\inference
  {}
  {\isCompatible{\tref{b}{p_1}}{\tref{b}{p_2}}}
  [\cbase]

&

\inference
  {\isCompatible{\tau_{x_1}}{\tau_{x_2}} &&
   \isCompatible{\tau_1}{\tau_2}}
  {\isCompatible{\tfun{x_1}{\tau_{x_1}}{\tau_1}}{\tfun{x_2}{\tau_{x_2}}{\tau_2}}}
  [\cfunction]
\end{array}$$



\medskip \judgementHead{Type Checking}{$\hastype{\Gamma}{e}{\tau}$}
$$\begin{array}{cc}

\inference
  {\isWellFormed{\Gamma}{\tau}}
  {\hastype{\Gamma}{\elabel{l}}{\tau}}
  [\tlabel]

&

\inference
  {\isWellFormed{\Gamma}{\tau_1} &&
   \isWellFormed{\Gamma}{\tau_2} && 
   \isCompatible{\tau_1}{\tau_2}}
  {\hastype{\Gamma}{\ecast{\tau_1}{\tau_2}{}}{\tfun{}{\tau_1}{\tau_2}}}
  [\tcast]
\end{array}$$


\caption{\textbf{Static Semantics: Well-formedness, Subtyping and Type Checking}}
\label{fig:rules}
\end{figure}




\subsection{Algorithmic Type Checking}

The great disadvantage of $\lambda^{\textsc{CON}}$ is that all contracts are checked in runtime, 
so type checking consumes cycles that otherwise would perform useful computation.
Moreover limited coverage is provided: contracts are checked only for data values and
code paths of actual execution.
This disadvantages are eliminated in Flanagan's Hybrid Type Checking \cite{Flanagan06}.
In their system, $\lambda^H$, they encoded contracts as dependent types:


Instead of type casting they used subtyping to convert values from one type to another.
Which in turn reduces to implication checking.
Their type system checks implications statically, whenever possible
and dynamically, only when necessary:


Consider our pred example::

\begin{verbatim}
pred :: x:{v : Int | v > 0} -> {v:Int | v = x - 1}
pred x = x - 1
\end{verbatim}

Say that we apply this function to the integer 8
and we can assume that  for all integers ty(n) = {v:Int | v = n}.

For this application to typecheck, we should prove that 
$$ {v:Int | v = 8} <: {v : Int | v > 0} $$
which reduces to 
$$ v = 8 \Rightarrow v > 0 $$
which is easy to prove by any algorithm that uses linear arithmentic.

But this reasining may create implications harder to prove.

They assume that 
there exists an algorithm that within limited time can
conservatively approximate implications between predicates.
They apply this algorithm to each implication and there are three cases:
\begin{itemize}
\item The algorithm proves that $E \vdash s \Rightarrow t$, this the relates contract
will always succeed.
\item The algorithm proves that $E \nvdash s \Rightarrow t$, this the relates contract
will always fails and the program is rejected..
\item The algorithm can not prove any of the above.
Thus, the expression is annotated with a type cast $\langle T_e \vartriangleright T_f\rangle e$
to dynamically ensure that values returned by $e$ are actually of the desired type $T_f$.
\end{itemize}

So actually they trnsform the program to add all required type casts, but they prove that
the original and the transformed program are ``bisimilar'', ie they behave equivallent with respect to 
operational semantics.

Finally he proves that for any static type checker S, it is possible to 
develop a hybrid type checker H
that performs somewhat better than S in the following sense:
\begin{itemize}
\item H dynamically detects errors that would be missed by S, since
H supports more precise specifications than S and can detect
violations of these specifications dynamically.
\item H statically detects all errors that would be detected by S, since
H can statically perform the same reasoning as S.
\item H actually detects more errors statically than S, since H sup-
ports more precise specifications, and could reasonably detect
some violations of these precise specifications statically.
\end{itemize}

\subsubsection{Formal Language}

The syntax of $\lambda_H$ is the same as $\lambda_C$, we only add the substyping rules.
As we stated, the source program goes through a transformation and the nessecary casts are added.
Thus, every time the \tsub rule is used, it is guaranteened that their algorithm can prove
the subtyping relation.

\begin{figure}[ht!]
\medskip \judgementHead{Subtyping}{\isSubType{\Gamma}{\tau_1}{\tau_2}}

$$
\inference
   {\Gamma, v:b \vdash  p1 \Rightarrow p2 }
   {\isSubType{\Gamma}{\tref{b}{p_1}}{\tref{b}{p_2}}}
   [\tsubBase]
$$

$$
\inference
   {\isSubType{\Gamma}{\tau_2}{\tau_1} &
	\isSubType{\Gamma, x_2:{\tau_2}}{\SUBST{\tau_1'}{x_1}{x_2}}{\tau_2'}	
   }
   {\isSubType{\Gamma}
	  {\tfun{x_1}{\tau_1}{\tau_1'}}
	  {\tfun{x_2}{\tau_2}{\tau_2'}}
}[\tsubFun]
$$


\medskip \judgementHead{Type Checking}{$\hastype{\Gamma}{e}{\tau}$}

$$\begin{array}{cc}

\inference
  {  \hastype{\Gamma}{e}{\tau_2} && \isSubType{\Gamma}{\tau_2}{\tau_1} 
  && \isWellFormed{\Gamma}{\tau_1}
  }
  {\hastype{\Gamma}{e}{\tau_1}}
  [\tsub]
\end{array}$$

\caption{\textbf{Static Semantics: Well-formedness, Subtyping and Type Checking}}
\label{fig:rules}
\end{figure}

 
