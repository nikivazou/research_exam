\newcommand\tlabel{T-Label}
\newcommand\tcast{T-Cast}

\newcommand\cbase{C-Base}
\newcommand\cfunction{C-Fun}


\newcommand\elabel[1]{\ensuremath{\Uparrow \ #1}}
\newcommand\ecast[2]{\ensuremath{\langle #1 \Rightarrow #2 \rangle}}

\newcommand\isCompatible[2]{\ensuremath{ #1 \Vert #2 }}



\section{Interactive theorem Proving}

% intro

One approach to verify that a program satisfies some 
specifications is to statically prove them.
% 
This approach is used by \textit{interactive theorem provers}, 
such as Coq, Agda and Isabelle,
that allow the 
expression of mathematical assertions, 
mechanically check proofs of these assertions, 
help to find formal proofs, 
and extract a certified program from 
the constructive proof of its formal specification.

% overview 

As an example, consider a function @pred@ in Coq, 
that computes the predecessor of a positive number,
and has type signature

@pred :: s:{n : nat | n > 0}  -> {v:nat | s = S v}@

This type signature says that if @pred@ is called with a 
positive number @s@, it will return @s@'s predecessors. 
There are two different assertions that should be proved:
\begin{itemize}
\item The result of the function is the predecessor of the argument.
At @pred@'s definitions the programmer should provide a proof
that this assertion is indeed satisfied.
\item The argument is a positive number.
At each call side of @pred@, 
the user should provide a proof that 
its argument is positive.
\end{itemize}

% implementation

In more details @pred@ function can be defined in Coq as follows \cite{cpdt}  
$$\centering
\begin{tabular}{c}
\begin{code}
Definition pred (s : {n : nat | n > 0}) : {m : nat | proj1_sig s = S m} :=
  match s return {m : nat | proj1_sig s = S m} with
    | exist 0 pf => match zgtz pf with end
    | exist (S n') pf => exist _ n' (eq_refl (S n'))
  end.
\end{code}
\end{tabular}
$$

where the refinement type is syntactic sugar,
defined in the standard library, for the type family @sig@:

$$\centering
\begin{tabular}{c}
\begin{code}
Inductive sig (A : Type) (P : A -> Prop) : Type :=
    exist : forall x : A, P x -> sig P
Notation
  "{ x : A | P }" := sig (fun x : A => P)
\end{code}
\end{tabular}
$$

The function @pred@ takes an argument @s@ which has two components:
a natural number @n@
and a proof @pf@ that this number is positive.
Then, there is a case analysis on @s@ and if @n@ is zero, then 
the proof @pf@ is used to reach a contradiction; thus this case can not occur.
Otherwise, @n@ has a predecessor, say @n'@ and 
the function returns @n'@ combined with a proof that its successor is 
equal to @n@. This proof is constructed by applying @eq_relf@, the only constructor
of equality to @S n'@.


\begin{comment}
where zgtz

\begin{verbatim}
Lemma zgtz : 0 > 0 -> False.
  crush.
Qed.

Inductive eq (A:Type) (x:A) : A -> Prop :=
    eq_refl : x = x :>A
\end{verbatim}
\end{comment}

As we said, at the call side of @pred@, the programmer should provide
both the argument and a proof that it is positive.
As an example, we can have 

$$\centering
\begin{tabular}{c}
\begin{code}
pred (exist _ 2 two_gt0)
\end{code}
\end{tabular}
$$
 
where @two_gt0@ is a proof that two is greater than zero.

\begin{comment}
Theorem two_gt0 : 2 > 0.
  crush.
Qed.
\end{comment}

Even this example seems tedious, 
interactive theorem prooving can be simplified
using inference and tacticks. 
%
Though, the user still needs to provide proofs.
%
We will discuss other systems, which remove this burden from the user.

\subsection{Manifest Contracts}

\subsubsection{Contracts}
Contracts are dynamically enforced pre- and post assertions.
and they play an important role in the construction of robust soft- 
ware. 
Their use in programming languages dates back to the 1970s. 
Eiffel \cite{Eiffel}, an object-oriented programming language,
totally adopted assertions and developed the 
``Design by Contract'' philosophy \cite{Meyer92}.

 
\paragraph*{Contracts for Higher-Order Functions}
Findler and Felleisen in \cite{Findler02} were the first to
integrate contracts in higher order languages.




Findler and Felleisen  created the first type system with contracts, $\lambda^{\textsc{CON}}$ 
where the expressions are extended with ``obligations'':
$e^{E, x_1, x_2}$
The first superscript is a contract expression that the base expression
is obliged to meet. The last two are variables. The variables enable
the contract monitoring system to assign blame properly. The first
variable names the party responsible for values that are produced by
the expression under the superscript (covariant) and the second variable names
the party responsible for values that it consumes(contravariant).


The above two basic characteristics, are summarized in the obligation reduction rules:
$$
V_1^{V_2, p, n} \longrightarrow if \ V_2(V_1) \ then \  V_1  \ else \ blame(``p'')
$$

$$
V_1^{V_3\longmapsto V_4, p, n} \longrightarrow (V_1 V_2^{V_3, n, p})^{V_4, p, n}
$$

The first rule shows how contracts are checked for first order values, while
the second rules is the enforcement of the ``even-odd'' rule.

Other features or their system are the two following:
\begin{itemize}
\item they allow dependent contracts, ie of the form $int [>0] \rightarrow int [\lambda x \lambda v. v >x]$
that describes a function that gives a positive integer $x$ it returns a value greater than $x$
\item they treat contracts as first class values, ie, contracts are values that can be passed to and from functions
\end{itemize}


Soundness and completeness \cite{BlumeM06}

\begin{verbatim}
Manifest Constracts Systems use casts , <Int => {v:Int | v > 0}> to convert 
values from one type to another,
(the left-
hand side is the source type and the right-hand side is the target type)

For
base contracts, a cast will behave just like a check on the target type: ap-
plied to n, the cast either returns n or raises \\l . 

A function cast ( T11 ->
T12 -> T21 -> T22 l v ) v will reduce to T12 -> T22 l (v ( T21 -> T11 l v )),
wrapping the argument v in a (contravariant) cast between the domain types
and wrapping the result of the application in a (covariant) cast between the
codomain types.

\end{verbatim}


$$
pred ::x:{v:Int | v > 0} -> {v:Int | v = x - 1} 
$$
\begin{verbatim}
pred' x = x - 1
pred = <Int -> Int => x:{v:Int | v > 0} -> {v:Int | v = x - 1}> pred'
\end{verbatim}

\begin{verbatim}
pred-> 
  (<Int -> Int => x:{v:Int | v > 0} -> {v:Int | v = x - 1}>p (\x -> x - 1) ) ->  
  \y:{v:Int | v > 0} (<Int => {v:Int | v = y-1} >p  ((\x -> x - 1) (<{v:Int | v > 0 }  => Int> y) ) ) 
  \y:{v:Int | v > 0} (<Int => {v:Int | v = y-1} >p  y-1 ) ) 
  
pred 0  -> no type check!
pred (<Int => {v:Int | v > 0 }u> 0)  -> || u


pred (<Int => {v:Int | v > 0 }u> 9)  -> 
   pred 9 -> 
  \y:{v:Int | v > 0} (<Int => {v:Int | v = y-1} >  y-1 ) ) 9
  <Int => {v:Int | v = 9-1} >  9-1 )
  <Int => {v:Int | v = 8} >  8 )
  8


badpred 9 -> 
  \y:{v:Int | v > 0} (<Int => {v:Int | v = y-1} >  y-3 ) ) 9
  <Int => {v:Int | v = 9-1} >  6 )
  <Int => {v:Int | v = 8} >p  6 )
  ||p

\end{verbatim}

\subsubsection{Formal Language}

In the expressions of our language we should add a blaming expression
and a type casting.
As a refinement the language can use any core expression.
Everything else remains unchanged.


In the typing judgements we add two rules:
a blame expression can have any well formed types, 
while a type casting expression behaves as a function from the 
source to the target type.
For a casting expression to typecheck, both types should be 
well formed and compatible, ie, their unrefined types should be 
the same. We check this through a new compatibility judgement.

\begin{figure}[t!]
\centering
$$
\begin{array}{rrcl}
\emphbf{Expressions} \quad 
  & e 
  & ::= \dots
  \spmid \elabel{l} 
  \spmid \ecast{\tau}{\tau} 
  \\[0.05in] 

\emphbf{Predicates} \quad 
  & p
  & ::= e
  & \dots
  \\[0.05in] 

\end{array}
$$
\caption{\textbf{Syntax of Expressions, Types and Schemas}}
\label{fig:syntax}
\end{figure}


\begin{figure}[ht!]

\medskip \judgementHead{Compatibility}{$\isCompatible{\tau_1}{\tau_2}$}
$$\begin{array}{cc}

\inference
  {}
  {\isCompatible{\tref{b}{p_1}}{\tref{b}{p_2}}}
  [\cbase]

&

\inference
  {\isCompatible{\tau_{x_1}}{\tau_{x_2}} &&
   \isCompatible{\tau_1}{\tau_2}}
  {\isCompatible{\tfun{x_1}{\tau_{x_1}}{tau_1}}{\tfun{x_2}{\tau_{x_2}}{\tau_2}}}
  [\cfunction]
\end{array}$$



\medskip \judgementHead{Type Checking}{$\hastype{\Gamma}{e}{\tau}$}
$$\begin{array}{cc}

\inference
  {\isWellFormed{\Gamma}{\tau}}
  {\hastype{\Gamma}{\elabel{l}}{\tau}}
  [\tlabel]

&

\inference
  {\isWellFormed{\Gamma}{\tau_1} &&
   \isWellFormed{\Gamma}{\tau_2} && 
   \isCompatible{\tau_1}{\tau_2}}
  {\hastype{\Gamma}{\ecast{\tau_1}{\tau_2}}{\tfun{}{\tau_1}{\tau_2}}}
  [\tcast]
\end{array}$$


\caption{\textbf{Static Semantics: Well-formedness, Subtyping and Type Checking}}
\label{fig:rules}
\end{figure}




\subsection{Algorithmic Type Checking}

The great disadvantage of $\lambda^{\textsc{CON}}$ is that all contracts are checked in runtime, 
so type checking consumes cycles that otherwise would perform useful computation.
Moreover limited coverage is provided: contracts are checked only for data values and
code paths of actual execution.
This disadvantages are eliminated in Flanagan's Hybrid Type Checking \cite{Flanagan06}.
In their system, $\lambda^H$, they encoded contracts as dependent types:


Instead of type casting they used subtyping to convert values from one type to another.
Which in turn reduces to implication checking.
Their type system checks implications statically, whenever possible
and dynamically, only when necessary:


Consider our pred example::

\begin{verbatim}
pred :: x:{v : Int | v > 0} -> {v:Int | v = x - 1}
pred x = x - 1
\end{verbatim}

Say that we apply this function to the integer 8
and we can assume that  for all integers ty(n) = {v:Int | v = n}.

For this application to typecheck, we should prove that 
$$ {v:Int | v = 8} <: {v : Int | v > 0} $$
which reduces to 
$$ v = 8 \Rightarrow v > 0 $$
which is easy to prove by any algorithm that uses linear arithmentic.

But this reasining may create implications harder to prove.

They assume that 
there exists an algorithm that within limited time can
conservatively approximate implications between predicates.
They apply this algorithm to each implication and there are three cases:
\begin{itemize}
\item The algorithm proves that $E \vdash s \Rightarrow t$, this the relates contract
will always succeed.
\item The algorithm proves that $E \nvdash s \Rightarrow t$, this the relates contract
will always fails and the program is rejected..
\item The algorithm can not prove any of the above.
Thus, the expression is annotated with a type cast $\langle T_e \vartriangleright T_f\rangle e$
to dynamically ensure that values returned by $e$ are actually of the desired type $T_f$.
\end{itemize}

So actually they trnsform the program to add all required type casts, but they prove that
the original and the transformed program are ``bisimilar'', ie they behave equivallent with respect to 
operational semantics.

Finally he proves that for any static type checker S, it is possible to 
develop a hybrid type checker H
that performs somewhat better than S in the following sense:
\begin{itemize}
\item H dynamically detects errors that would be missed by S, since
H supports more precise specifications than S and can detect
violations of these specifications dynamically.
\item H statically detects all errors that would be detected by S, since
H can statically perform the same reasoning as S.
\item H actually detects more errors statically than S, since H sup-
ports more precise specifications, and could reasonably detect
some violations of these precise specifications statically.
\end{itemize}

\subsubsection{Formal Language}

The syntax of $\lambda_H$ is the same as $\lambda_C$, we only add the substyping rules.
As we stated, the source program goes through a transformation and the nessecary casts are added.
Thus, every time the \tsub rule is used, it is guaranteened that their algorithm can prove
the subtyping relation.

\begin{figure}[ht!]
\medskip \judgementHead{Subtyping}{\isSubType{\Gamma}{\tau_1}{\tau_2}}

$$
\inference
   {\Gamma, v:b \vdash  p1 \Rightarrow p2 }
   {\isSubType{\Gamma}{\tref{b}{p_1}}{\tref{b}{p_2}}}
   [\tsubBase]
$$

$$
\inference
   {\isSubType{\Gamma}{\tau_2}{\tau_1} &
	\isSubType{\Gamma, x_2:{\tau_2}}{\SUBST{\tau_1'}{x_1}{x_2}}{\tau_2'}	
   }
   {\isSubType{\Gamma}
	  {\tfun{x_1}{\tau_1}{\tau_1'}}
	  {\tfun{x_2}{\tau_2}{\tau_2'}}
}[\tsubFun]
$$


\medskip \judgementHead{Type Checking}{$\hastype{\Gamma}{e}{\tau}$}

$$\begin{array}{cc}

\inference
  {  \hastype{\Gamma}{e}{\tau_2} && \isSubType{\Gamma}{\tau_2}{\tau_1} 
  && \isWellFormed{\Gamma}{\tau_1}
  }
  {\hastype{\Gamma}{e}{\tau_1}}
  [\tsub]
\end{array}$$

\caption{\textbf{Static Semantics: Well-formedness, Subtyping and Type Checking}}
\label{fig:rules}
\end{figure}

 
